"""Module for the various Tokenizers usuable by Clusterer."""
import re
import string


class Tokenizer:
  """Class for our general suite of string tokenizers for our Clusterer."""
  _JAVA_CLASS_LINE_PREFIX = r'\s+at'

  def __init__(self, config):
    """Initializes the information needed by Tokenizer.

    Args:
      config: config_pb2 proto specified by the configuration file
    """
    self.min_token_len = config.clusterer.tokenizer.token_min_length
    # Additional splitting only makes sense on human readable mode
    self.split_ons = config.clusterer.tokenizer.split_on
    self.punctuations = config.clusterer.tokenizer.punctuation
    self.ignore_tokens = [
        token.lower()
        for token in config.clusterer.tokenizer.ignore_token_matcher
    ]

  def human_readable_tokenizer(self, input_string):
    """Tokenization method for parsing the input_string into a human readable list of strings.

    Args:
      input_string: str we are attempting to tokenize

    Returns:
      List[str], each string representing a human readable token
    """
    # in case input is stored in a different format in dataframe
    input_string = str(input_string)
    # filtering all all java class lines out
    input_string_lines = input_string.splitlines()
    input_strings_filtered = [
        s for s in input_string_lines
        if not re.search(self._JAVA_CLASS_LINE_PREFIX, s)
    ]
    input_string = '\n'.join(input_strings_filtered)
    # Base split on new line and spaces
    tokens = input_string.split()
    # Split for every other defined additional splitter
    for split_on in self.split_ons:
      # pylint: disable=cell-var-from-loop
      tokens = sum(map(lambda w: re.split(split_on, w), tokens), [])
    # Remove all lines that contain '.' (extraneous class info)
    # Examples include embedded class 'com.google.net.rpc3.RpcException:'
    tokens = [w for w in tokens if not re.search(r'\.', w)]
    # Remove all lines that contain ';' (extraneous debug info)
    # Examples include 9;StartTimeMs which many exceptions have embedded
    tokens = [w for w in tokens if not re.search(';', w)]
    # Removing trailing and leading punctuation
    punctuation = string.punctuation + ''.join(self.punctuations)
    tokens = list(map(lambda w: w.strip(punctuation), tokens))
    # Removing all numerical hex values
    # WARNING this regex actually matches to certain words like 'be' since
    # 'be' is indeed a hex value
    numerics_regex = r'[0-9a-f|:|\.|\[|\]]+'
    # pylint: disable=cell-var-from-loop
    tokens = [w for w in tokens if not re.fullmatch(numerics_regex, w)]
    # Remove 1 word characters
    tokens = [w for w in tokens if len(w) >= self.min_token_len]
    # Remove empty strings
    tokens = [w for w in tokens if w]
    # lowercase all words for consistency
    tokens = list(map(lambda w: w.lower(), tokens))
    # filter out undesired tokens
    filtered_tokens = []
    for token in tokens:
      if token not in self.ignore_tokens:
        filtered_tokens.append(token)
    return filtered_tokens

  def stack_trace_line_tokenizer(self, input_string):
    """Tokenization method for parsing the input_string into a list of stack trace strings.

    Note, each token representing a line in the stack trace.

    Args:
      input_string: str we are attempting to tokenize

    Returns:
      List[str], each string representing a stack trace class line
    """
    # in case input is stored in a different format in dataframe
    input_string = str(input_string)
    input_string_lines = input_string.splitlines()
    # search for stack trace lines, i.e those that begin with '\tat'
    stack_lines = []
    for line in input_string_lines:
      prefix_search = re.search(self._JAVA_CLASS_LINE_PREFIX, line)
      if prefix_search:
        # we only care about the "class" or anything after the
        # \tat and before line number
        stack_lines.append(line[prefix_search.end() + 1:line.index('(')])

    # filter out minimum length tokens
    stack_lines = [
        line for line in stack_lines if len(line) >= self.min_token_len
    ]

    filtered_lines = []
    # filter out undesired tokens
    for line in stack_lines:
      if line not in self.ignore_tokens:
        filtered_lines.append(line)

    return filtered_lines

  def combined_tokenizer(self, input_string):
    """Tokenization method for parsing input_string into list of tokens.

    Tokens are either human readable strings as generated from human_readable_tokenizer,
    or stack trace lines generated by stack_trace_line_tokenizer.

    Args:
      input_string: str we are attempting to tokenize

    Returns:
      List[str] tokens, each token representing either a human readable word or a stack line
    """
    return self.human_readable_tokenizer(
        input_string) + self.stack_trace_line_tokenizer(input_string)
